{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "85968a13",
      "metadata": {
        "id": "85968a13"
      },
      "source": [
        "# Описание ДЗ1."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39ba428c",
      "metadata": {
        "id": "39ba428c"
      },
      "source": [
        "На основе семинара 1 предложите 2 метода улучшения построения эмбеддингов вопросов на основе word vectors.\n",
        "\n",
        "За задание можно получить максимум 10 баллов (за каждый метод можно получить максимум 5 баллов).\n",
        "\n",
        "Разбалловка:\n",
        "*   **Воспроизводимость и читабельность кода - 1 балл** (все воспроизвелось и все понятно для проверяющего - 1 балл; непонятный код и/или ничего не воспроизвелось - 0 баллов).\n",
        "*   **Корректность метода - 1 балл** (метод математически корректен и с точки зрения логики кода нет ошибок - 1 балл; все остальные случаи - 0 баллов).\n",
        "*   **Описание метода в техническом отчете - 2 балла** (есть подробное описание метода и почему используете именно его с обоснованиями - 2 балла; есть описание метода, но нет обоснования - 1 балл; нет описания метода- 0 баллов).\n",
        "*   **Иновационность - 1 балл** (используете нетривиальную обработку/лосс-функцию/модель - 1 балл; просто перебираете модели из коробок - 0 баллов).\n",
        "\n",
        "!!! ДЗ необходимо выполнять только в Google Colab !!!\n",
        "\n",
        "Присылать на почту llmrisks@yandex.ru с номером ДЗ и ФИО в теме. Каждая ДЗ отдельным письмом с отдельной темой."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5510344",
      "metadata": {
        "id": "e5510344"
      },
      "source": [
        "# 1. Информация о сабмите"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c1f039a",
      "metadata": {
        "id": "5c1f039a"
      },
      "source": [
        "**Мартин Михаил Алексеевич**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae86fd35",
      "metadata": {
        "id": "ae86fd35"
      },
      "source": [
        "# 2. Технический отчет"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i8BcqJXtrm8E",
      "metadata": {
        "id": "i8BcqJXtrm8E"
      },
      "source": [
        "Методы улучшения построения эмбеддингов фраз на основе эмбеддингов входящих слов, использованные в данном домашнем задании:\n",
        "1. Обработка входного текста  \n",
        "   Мотивация: Так как размер обучающей выборки мал, хотелось ужать словарь токенов, удалив шумные токены и объеденив очень похожие.\n",
        "\n",
        "   - Приводим все символы в словах к строчной форме (объединение схожих)  \n",
        "   - Удаляем [стоп-слова](https://en.wikipedia.org/wiki/Stop_word) (удалаяем шумные)  \n",
        "   - Удаляем знаки препинания и прочие символы (удаляем шумные)  \n",
        "   - [Лемматизируем](https://ru.wikipedia.org/wiki/Лемматизация) токены, т.е. приводим их к нормальной форме (объеденение схожих)\n",
        "\n",
        "2. Обработка эмбеддингов входящих слов  \n",
        "   Мотивация: Когда мы усредняем эмбеддинги, обычно считается, что каждый токен имеет равный вес, но на самом деле это не всегда справедливо. Лучше использовать взвешенное среднее, где ключевым словам присваивается больший вес. Так можно точнее передать значимость ключевых слов в эмбеддинге всего предложения.  \n",
        "   Для этого можно воспользоваться методом [TF-IDF](https://ru.wikipedia.org/wiki/TF-IDF), делая акцент на весе IDF, поскольку частота появления слов (TF) уже учитывается при суммировании эмбеддингов.\n",
        "\n",
        "\n",
        "$$\n",
        "\\text{IDF}(w, D) = \\log\\left(\\frac{|D|}{|{d∈D:w∈d|}}\\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46f17064",
      "metadata": {
        "id": "46f17064"
      },
      "source": [
        "# 3. *Code*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c21fafa",
      "metadata": {
        "id": "7c21fafa"
      },
      "source": [
        "## 3.1 Импорт библиотек"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0b83137c",
      "metadata": {
        "id": "0b83137c"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tqdm import tqdm\n",
        "from pymystem3 import Mystem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "_AIb56A6hXtc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AIb56A6hXtc",
        "outputId": "925b771d-553e-46d5-8428-ff326208237d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaf6551c",
      "metadata": {
        "id": "eaf6551c"
      },
      "source": [
        "## 3.2 Загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3c73eb32",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c73eb32",
        "outputId": "ac9bcabb-f32a-4dd8-cfa2-97e4d1260ade"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-13 14:40:22--  https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.dropbox.com/scl/fi/p0t2dw6oqs6oxpd6zz534/quora.txt?rlkey=bjupppwua4zmd4elz8octecy9&dl=1 [following]\n",
            "--2024-12-13 14:40:23--  https://www.dropbox.com/scl/fi/p0t2dw6oqs6oxpd6zz534/quora.txt?rlkey=bjupppwua4zmd4elz8octecy9&dl=1\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc783b2d262895f7c97e29e875fe.dl.dropboxusercontent.com/cd/0/inline/CgJiZ7w1gAAqUJCnG_OixDJFqODy6gsMTG5-Jkf0u8ZKZgYi1yOAQeZckj8j9hwx249HWjCb8bAqAlGttnhDMwrxnapc6g6YpNJl0SB_mkux65QfCnoA1MrRIGY6gcdeLE8/file?dl=1# [following]\n",
            "--2024-12-13 14:40:23--  https://uc783b2d262895f7c97e29e875fe.dl.dropboxusercontent.com/cd/0/inline/CgJiZ7w1gAAqUJCnG_OixDJFqODy6gsMTG5-Jkf0u8ZKZgYi1yOAQeZckj8j9hwx249HWjCb8bAqAlGttnhDMwrxnapc6g6YpNJl0SB_mkux65QfCnoA1MrRIGY6gcdeLE8/file?dl=1\n",
            "Resolving uc783b2d262895f7c97e29e875fe.dl.dropboxusercontent.com (uc783b2d262895f7c97e29e875fe.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uc783b2d262895f7c97e29e875fe.dl.dropboxusercontent.com (uc783b2d262895f7c97e29e875fe.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33813903 (32M) [application/binary]\n",
            "Saving to: ‘./quora.txt’\n",
            "\n",
            "./quora.txt         100%[===================>]  32.25M  54.5MB/s    in 0.6s    \n",
            "\n",
            "2024-12-13 14:40:24 (54.5 MB/s) - ‘./quora.txt’ saved [33813903/33813903]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download the data:\n",
        "!wget https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1 -O ./quora.txt\n",
        "# alternative download link: https://yadi.sk/i/BPQrUu1NaTduEw\n",
        "\n",
        "data = open(\"./quora.txt\", encoding=\"utf-8\").read().split(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a62af06",
      "metadata": {
        "id": "5a62af06"
      },
      "source": [
        "## 3.3. Модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "DM4v7-DF0IgC",
      "metadata": {
        "id": "DM4v7-DF0IgC"
      },
      "outputs": [],
      "source": [
        "model = api.load(\"glove-twitter-100\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9IZPNI93uy4M",
      "metadata": {
        "id": "9IZPNI93uy4M"
      },
      "outputs": [],
      "source": [
        "class QA_Helper:\n",
        "\n",
        "    def __init__(self, tokenizer, stop_words, lemmer, model, tfidf_vectorizer) -> None:\n",
        "        self.tokenizer = tokenizer\n",
        "        self.stop_words = stop_words\n",
        "        self.lemmer = lemmer\n",
        "        self.model = model\n",
        "        self.tfidf_vectorizer = tfidf_vectorizer\n",
        "        self.train_data = None\n",
        "\n",
        "    def fit(self, data: list[str]) -> None:\n",
        "\n",
        "        # перед расчётом TF-IDF предобрабатываем данные\n",
        "        data = self.preprocessed(data)\n",
        "        # обучаем TF-IDF модуль\n",
        "        self.tfidf_vectorizer.fit(data)\n",
        "        # сохраняем IDF\n",
        "        self.idfs = {\n",
        "            word: self.tfidf_vectorizer.idf_[i]\n",
        "            for i, word in enumerate(self.tfidf_vectorizer.get_feature_names_out())\n",
        "        }\n",
        "        # находим эмбеддинги для обучающих данных\n",
        "        self.train_data = self.get_phrase_embeddings(data)\n",
        "\n",
        "    def get_phrase_embeddings(self, phrase: list[str] | str) -> list[np.ndarray] | np.ndarray:\n",
        "\n",
        "        if isinstance(phrase, list):\n",
        "            return list(map(self.get_phrase_embeddings, phrase))\n",
        "        else:\n",
        "            phrase = self.preprocessed(phrase)\n",
        "\n",
        "        # снова разбиваем на токены\n",
        "        phrase = self.tokenizer.tokenize(phrase)\n",
        "\n",
        "        # получаем эмбеддинги слов в фразе\n",
        "        words_embeddings = [self.model.get_vector(token) for token in phrase]\n",
        "        # преобразуем их в numpy-массив\n",
        "        words_embeddings = np.array(words_embeddings)\n",
        "\n",
        "        # подтягиваем IDF'ы для токенов в фразе\n",
        "        idfs = [self.idfs.get(token, 1) for token in phrase]\n",
        "        # преобразуем их в numpy-массив\n",
        "        idfs = np.array(idfs)[:, np.newaxis]\n",
        "\n",
        "        # получаем эмбеддинги фразы, как средневзвешенное эмбеддингов слов в фразе на IDF\n",
        "        if words_embeddings.size > 0:\n",
        "            phrase_embeddings = words_embeddings * idfs\n",
        "            phrase_embeddings = words_embeddings.mean(axis=0)\n",
        "        else:\n",
        "            phrase_embeddings = np.zeros([self.model.vector_size], dtype=\"float32\")\n",
        "\n",
        "        return phrase_embeddings\n",
        "\n",
        "    def preprocessed(self, phrase: list[str] | str) -> list[str] | str:\n",
        "\n",
        "        if isinstance(phrase, list):\n",
        "            return list(map(self.preprocessed, phrase))\n",
        "\n",
        "        # приводим к нижнему регистру\n",
        "        phrase = phrase.lower()\n",
        "        # разбиваем на токены\n",
        "        phrase = self.tokenizer.tokenize(phrase)\n",
        "        # отбрасываем стоп-слова\n",
        "        phrase = [token for token in phrase if token not in self.stop_words]\n",
        "        # отбрасываем знаки пунктуации и прочие символы\n",
        "        phrase = [token for token in phrase if token.isalpha()]\n",
        "        # лематизируем токены\n",
        "        phrase = self.lemmer.lemmatize(\" \".join(phrase))\n",
        "        # отбрасывем токены, которые не знает модель\n",
        "        phrase = self.tokenizer.tokenize(phrase)\n",
        "        phrase = [token for token in phrase if token in self.model.key_to_index]\n",
        "        phrase = \" \".join(phrase)\n",
        "\n",
        "        return phrase\n",
        "\n",
        "    def find_nearest(self, query, k: int = 10) -> list[str]:\n",
        "        \"\"\"\n",
        "        Для данного вопроса возвращает k наиболее похожих на него вопросов.\n",
        "\n",
        "        Возвращаемые вопросы упорядочены по убыванию схожести.\n",
        "        Под схожестью понимается косинусное расстояние между векторами-эмбеддингами.\n",
        "\n",
        "        \"\"\"\n",
        "        # получаем эмбеддинг данного вопроса\n",
        "        embedding_query = self.get_phrase_embeddings(query)\n",
        "        # находим вектор схожести с обучающими данными\n",
        "        similarities = np.array([\n",
        "            1 - cosine(embedding_query, embedding_data)\n",
        "            for embedding_data in tqdm(self.train_data)\n",
        "        ])\n",
        "        # вытаскиваем топ-k схожих вопросов\n",
        "        indices = np.argsort(similarities)[::-1]\n",
        "        indices = np.argpartition(-similarities, range(10))[:k]\n",
        "\n",
        "        return [data[i] for i in indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Тестирование"
      ],
      "metadata": {
        "id": "t5NehQjXfhJ3"
      },
      "id": "t5NehQjXfhJ3"
    },
    {
      "cell_type": "code",
      "source": [
        "qa_helper = QA_Helper(\n",
        "    tokenizer=WordPunctTokenizer(),\n",
        "    stop_words=stopwords.words(\"english\"),\n",
        "    lemmer=WordNetLemmatizer(),\n",
        "    model=model,\n",
        "    tfidf_vectorizer=TfidfVectorizer(),\n",
        ")\n",
        "qa_helper.fit(data)"
      ],
      "metadata": {
        "id": "UIg02zdVdHBE"
      },
      "id": "UIg02zdVdHBE",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cnNVoCnT4ldL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnNVoCnT4ldL",
        "outputId": "105bb56a-7746-45b3-be84-07f760a7b794"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/537273 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/scipy/spatial/distance.py:647: RuntimeWarning: invalid value encountered in divide\n",
            "  dist = 1.0 - uv / math.sqrt(uu * vv)\n",
            "100%|██████████| 537273/537273 [00:06<00:00, 81961.06it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['How do I enter color codes in excel?',\n",
              " 'Where do I enter my free ride code for Lyft?',\n",
              " 'What should I do to enter hollywood?',\n",
              " 'How can I enter into Hollywood?',\n",
              " 'What is your review of Enter Contests to Win Prizes?',\n",
              " 'What is the best platform to enter in quiz competitions and earn?',\n",
              " 'Where can I get an ebook of A Quest of Heroes parts for free?',\n",
              " 'Can I use PlayStation VR Launch Bundle in other countries?',\n",
              " 'How did Disney enter India?',\n",
              " 'How do you enter boot menu in Lenovo?']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "qa_helper.find_nearest(\"How do i enter the matrix?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "eeKqGht-RUwY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeKqGht-RUwY",
        "outputId": "22e4e2a9-1656-44f6-c18d-d9d7ea401adb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 537273/537273 [00:08<00:00, 64666.06it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What is Trump?',\n",
              " 'What do we do now that Trump has won?',\n",
              " 'How trump won?',\n",
              " 'Donald trump won . What now?',\n",
              " 'Who is Donald Trump?',\n",
              " 'Is Donald Trump antifragile?',\n",
              " 'Is Donald Trump an isolationist?',\n",
              " 'Is Donald trump a populist?',\n",
              " 'Will Donald Trump destroy Donald Trump?',\n",
              " 'How did Melania Trump meet Donald Trump?']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "qa_helper.find_nearest(\"How does Trump?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "C0hP2E2TRcBZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0hP2E2TRcBZ",
        "outputId": "592671c4-f8a0-45db-d42f-67b8b196eca5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 537273/537273 [00:07<00:00, 76596.75it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Why do I ask this question?',\n",
              " 'How do I ask a question on this?',\n",
              " 'How do I ask a question?',\n",
              " 'Can you ask me a question?',\n",
              " 'How do you ask a question?',\n",
              " 'Are there Quorans who ask a question and answer it themselves?',\n",
              " 'Why do we ask questions?',\n",
              " 'Why do we ask all these questions?',\n",
              " 'What questions to ask any drdummer?',\n",
              " 'What questions should we ask ourselves?']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "qa_helper.find_nearest(\"Why don't i ask a question myself?\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}